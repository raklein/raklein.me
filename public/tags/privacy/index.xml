<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>privacy on Richard Klein</title>
    <link>/tags/privacy/</link>
    <description>Recent content in privacy on Richard Klein</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="/tags/privacy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introducing DataCheck: A tool to automatically prevent privacy violations</title>
      <link>/2022/11/datacheck-rmd/</link>
      <pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/2022/11/datacheck-rmd/</guid>
      <description>


&lt;p&gt;Our prior work demonstrated that &lt;a href=&#34;https://www.nature.com/articles/s41562-022-01481-w#citeas&#34;&gt;5-10% of open datasets contain potential privacy violations&lt;/a&gt;, so we developed a tool to help. &lt;a href=&#34;(https://libscie.github.io/datacheck-website/)&#34;&gt;DataCheck&lt;/a&gt; can automatically scan a dataset and flag 14 types of common privacy violations before a researcher makes their data public. Available as both a &lt;a href=&#34;https://libscie.github.io/datacheck-website/&#34;&gt;Web app&lt;/a&gt;
and &lt;a href=&#34;https://github.com/libscie/datacheck&#34;&gt;R package&lt;/a&gt;, the tool runs locally so there is no risk of exposing participant data during the scan. It was validated on both live and simulated datasets to be &amp;gt;98% accurate.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Protecting Privacy in the Era of Open Science</title>
      <link>/2022/11/privacy-protection-rmd/</link>
      <pubDate>Fri, 04 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/2022/11/privacy-protection-rmd/</guid>
      <description>


&lt;p&gt;We just &lt;a href=&#34;https://www.nature.com/articles/s41562-022-01481-w#citeas&#34;&gt;published&lt;/a&gt; our findings and recommendations for researchers to better protect the privacy of research participants when sharing open data. Examining over 2,000 public datasets from papers using human participants, we found that 5-10% contained information that could potentially re-identify participants.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
